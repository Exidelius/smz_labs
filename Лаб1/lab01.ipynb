{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import ipytest\n",
    "import numpy as np\n",
    "import pytest\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.common_types import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution2D(\n",
    "    input: Tensor,\n",
    "    weights: Tensor,\n",
    "    bias: Optional[Tensor] = None,\n",
    "    stride: Optional[Union[int, Tuple]] = 1,\n",
    "    padding: Optional[Union[int, Union[Tuple, str]]] = 0,\n",
    "    dilation: Optional[Union[int, Tuple]] = 1,\n",
    "    groups: Optional[int] = 1,\n",
    ") -> Tensor:\n",
    "    batch_size, in_channels, input_height, input_width = input.shape\n",
    "    out_channels, in_channels_groups, weights_height, weights_width = weights.shape\n",
    "\n",
    "    input = nn.functional.pad(input, (padding, padding, padding, padding))\n",
    "\n",
    "    result_height = (\n",
    "        input_height + 2 * padding - dilation * (weights_height - 1) - 1\n",
    "    ) // stride + 1\n",
    "    result_width = (\n",
    "        input_width + 2 * padding - dilation * (weights_width - 1) - 1\n",
    "    ) // stride + 1\n",
    "\n",
    "    grouped_channels = out_channels // groups if groups else out_channels\n",
    "\n",
    "    result = torch.zeros((batch_size, grouped_channels, result_height, result_width))\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        for channel in range(out_channels):\n",
    "            for i in range(0, input.shape[2] - dilation*(weights_height - 1) + 1, stride):\n",
    "                for j in range(\n",
    "                    0, input.shape[3] - dilation*(weights_width - 1) + 1, stride\n",
    "                ):\n",
    "                    for group in range(grouped_channels):\n",
    "                        d = input[\n",
    "                            batch, :, i : i + weights_height, j : j + weights_width\n",
    "                        ]\n",
    "                        result[batch, group, i // stride, j // stride] = (\n",
    "                            d * weights[channel]\n",
    "                        ).sum()\n",
    "            result[batch] += bias[channel] if bias else 0\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture(scope='class')\n",
    "def inputs():\n",
    "    return torch.randn(1, 2, 4, 4)\n",
    "\n",
    "@pytest.fixture(scope='class')\n",
    "def weights():\n",
    "    return torch.randn(1, 2, 3, 3)\n",
    "\n",
    "@pytest.fixture(scope='class')\n",
    "def bias():\n",
    "    return torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                        [100%]\u001b[0m\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_________________________________ TestConv2D.test_conv2d_success __________________________________\u001b[0m\n",
      "\n",
      "self = <__main__.TestConv2D object at 0x000002430FDFEED0>\n",
      "inputs = tensor([[[[-0.7242,  0.5520,  1.7713,  0.9649],\n",
      "          [-0.4341, -0.8966,  0.1245,  0.0645],\n",
      "          [ 0.0583, -0...,  1.3643,  0.4532],\n",
      "          [-1.3859, -0.4927,  0.9672, -0.6518],\n",
      "          [-2.3218, -1.5389, -1.1958,  0.3439]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_conv2d_success\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, inputs, weights):\u001b[90m\u001b[39;49;00m\n",
      ">       result = convolution2D(inputs, weights)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\106665805.py\u001b[0m:6: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "input = tensor([[[[-0.7242,  0.5520,  1.7713,  0.9649],\n",
      "          [-0.4341, -0.8966,  0.1245,  0.0645],\n",
      "          [ 0.0583, -0...,  1.3643,  0.4532],\n",
      "          [-1.3859, -0.4927,  0.9672, -0.6518],\n",
      "          [-2.3218, -1.5389, -1.1958,  0.3439]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = None, stride = 1, padding = 0, dilation = 1, groups = 1\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mconvolution2D\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        weights: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        bias: Optional[Tensor] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        stride: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        padding: Optional[Union[\u001b[96mint\u001b[39;49;00m, Union[Tuple, \u001b[96mstr\u001b[39;49;00m]]] = \u001b[94m0\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        dilation: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        groups: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        batch_size, in_channels, input_height, input_width = \u001b[96minput\u001b[39;49;00m.shape\u001b[90m\u001b[39;49;00m\n",
      "        out_channels, in_channels_groups, weights_height, weights_width = weights.shape\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m = nn.functional.pad(\u001b[96minput\u001b[39;49;00m, (padding, padding, padding, padding))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result_height = (\u001b[90m\u001b[39;49;00m\n",
      "            input_height + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_height - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result_width = (\u001b[90m\u001b[39;49;00m\n",
      "            input_width + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_width - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        grouped_channels = out_channels // groups \u001b[94mif\u001b[39;49;00m groups \u001b[94melse\u001b[39;49;00m out_channels\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = torch.zeros((batch_size, grouped_channels, result_height, result_width))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m batch \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(batch_size):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m channel \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(out_channels):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m2\u001b[39;49;00m] - dilation*(weights_height - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m3\u001b[39;49;00m] - dilation*(weights_width - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride\u001b[90m\u001b[39;49;00m\n",
      "                    ):\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mfor\u001b[39;49;00m group \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(grouped_channels):\u001b[90m\u001b[39;49;00m\n",
      "                            d = \u001b[96minput\u001b[39;49;00m[\u001b[90m\u001b[39;49;00m\n",
      "                                batch, :, i : i + weights_height, j : j + weights_width\u001b[90m\u001b[39;49;00m\n",
      "                            ]\u001b[90m\u001b[39;49;00m\n",
      "                            result[batch, group, i // stride, j // stride] = (\u001b[90m\u001b[39;49;00m\n",
      ">                               d * weights[channel]\u001b[90m\u001b[39;49;00m\n",
      "                            ).sum()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                           RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\303552282.py\u001b[0m:37: RuntimeError\n",
      "\u001b[31m\u001b[1m_______________________________ TestConv2D.test_conv2d_bias_success _______________________________\u001b[0m\n",
      "\n",
      "self = <__main__.TestConv2D object at 0x000002430FFFCE10>\n",
      "inputs = tensor([[[[-0.7242,  0.5520,  1.7713,  0.9649],\n",
      "          [-0.4341, -0.8966,  0.1245,  0.0645],\n",
      "          [ 0.0583, -0...,  1.3643,  0.4532],\n",
      "          [-1.3859, -0.4927,  0.9672, -0.6518],\n",
      "          [-2.3218, -1.5389, -1.1958,  0.3439]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_conv2d_bias_success\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, inputs, weights, bias):\u001b[90m\u001b[39;49;00m\n",
      ">       result = convolution2D(inputs, weights, bias)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\106665805.py\u001b[0m:11: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "input = tensor([[[[-0.7242,  0.5520,  1.7713,  0.9649],\n",
      "          [-0.4341, -0.8966,  0.1245,  0.0645],\n",
      "          [ 0.0583, -0...,  1.3643,  0.4532],\n",
      "          [-1.3859, -0.4927,  0.9672, -0.6518],\n",
      "          [-2.3218, -1.5389, -1.1958,  0.3439]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603]), stride = 1, padding = 0, dilation = 1, groups = 1\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mconvolution2D\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        weights: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        bias: Optional[Tensor] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        stride: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        padding: Optional[Union[\u001b[96mint\u001b[39;49;00m, Union[Tuple, \u001b[96mstr\u001b[39;49;00m]]] = \u001b[94m0\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        dilation: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        groups: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        batch_size, in_channels, input_height, input_width = \u001b[96minput\u001b[39;49;00m.shape\u001b[90m\u001b[39;49;00m\n",
      "        out_channels, in_channels_groups, weights_height, weights_width = weights.shape\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m = nn.functional.pad(\u001b[96minput\u001b[39;49;00m, (padding, padding, padding, padding))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result_height = (\u001b[90m\u001b[39;49;00m\n",
      "            input_height + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_height - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result_width = (\u001b[90m\u001b[39;49;00m\n",
      "            input_width + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_width - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        grouped_channels = out_channels // groups \u001b[94mif\u001b[39;49;00m groups \u001b[94melse\u001b[39;49;00m out_channels\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = torch.zeros((batch_size, grouped_channels, result_height, result_width))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m batch \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(batch_size):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m channel \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(out_channels):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m2\u001b[39;49;00m] - dilation*(weights_height - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m3\u001b[39;49;00m] - dilation*(weights_width - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride\u001b[90m\u001b[39;49;00m\n",
      "                    ):\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mfor\u001b[39;49;00m group \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(grouped_channels):\u001b[90m\u001b[39;49;00m\n",
      "                            d = \u001b[96minput\u001b[39;49;00m[\u001b[90m\u001b[39;49;00m\n",
      "                                batch, :, i : i + weights_height, j : j + weights_width\u001b[90m\u001b[39;49;00m\n",
      "                            ]\u001b[90m\u001b[39;49;00m\n",
      "                            result[batch, group, i // stride, j // stride] = (\u001b[90m\u001b[39;49;00m\n",
      ">                               d * weights[channel]\u001b[90m\u001b[39;49;00m\n",
      "                            ).sum()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                           RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\303552282.py\u001b[0m:37: RuntimeError\n",
      "\u001b[31m\u001b[1m___________________________ TestConv2D.test_conv2d_bias_padding_success ___________________________\u001b[0m\n",
      "\n",
      "self = <__main__.TestConv2D object at 0x000002430FFFBB90>\n",
      "inputs = tensor([[[[-0.7242,  0.5520,  1.7713,  0.9649],\n",
      "          [-0.4341, -0.8966,  0.1245,  0.0645],\n",
      "          [ 0.0583, -0...,  1.3643,  0.4532],\n",
      "          [-1.3859, -0.4927,  0.9672, -0.6518],\n",
      "          [-2.3218, -1.5389, -1.1958,  0.3439]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_conv2d_bias_padding_success\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, inputs, weights, bias):\u001b[90m\u001b[39;49;00m\n",
      ">       result = convolution2D(inputs, weights, bias, padding=\u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\106665805.py\u001b[0m:16: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "input = tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000,  0.000...0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603]), stride = 1, padding = 5, dilation = 1, groups = 1\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mconvolution2D\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        weights: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        bias: Optional[Tensor] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        stride: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        padding: Optional[Union[\u001b[96mint\u001b[39;49;00m, Union[Tuple, \u001b[96mstr\u001b[39;49;00m]]] = \u001b[94m0\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        dilation: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        groups: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        batch_size, in_channels, input_height, input_width = \u001b[96minput\u001b[39;49;00m.shape\u001b[90m\u001b[39;49;00m\n",
      "        out_channels, in_channels_groups, weights_height, weights_width = weights.shape\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m = nn.functional.pad(\u001b[96minput\u001b[39;49;00m, (padding, padding, padding, padding))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result_height = (\u001b[90m\u001b[39;49;00m\n",
      "            input_height + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_height - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result_width = (\u001b[90m\u001b[39;49;00m\n",
      "            input_width + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_width - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        grouped_channels = out_channels // groups \u001b[94mif\u001b[39;49;00m groups \u001b[94melse\u001b[39;49;00m out_channels\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = torch.zeros((batch_size, grouped_channels, result_height, result_width))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m batch \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(batch_size):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m channel \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(out_channels):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m2\u001b[39;49;00m] - dilation*(weights_height - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m3\u001b[39;49;00m] - dilation*(weights_width - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride\u001b[90m\u001b[39;49;00m\n",
      "                    ):\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mfor\u001b[39;49;00m group \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(grouped_channels):\u001b[90m\u001b[39;49;00m\n",
      "                            d = \u001b[96minput\u001b[39;49;00m[\u001b[90m\u001b[39;49;00m\n",
      "                                batch, :, i : i + weights_height, j : j + weights_width\u001b[90m\u001b[39;49;00m\n",
      "                            ]\u001b[90m\u001b[39;49;00m\n",
      "                            result[batch, group, i // stride, j // stride] = (\u001b[90m\u001b[39;49;00m\n",
      ">                               d * weights[channel]\u001b[90m\u001b[39;49;00m\n",
      "                            ).sum()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                           RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\303552282.py\u001b[0m:37: RuntimeError\n",
      "\u001b[31m\u001b[1m_______________________ TestConv2D.test_conv2d_bias_padding_stride_success ________________________\u001b[0m\n",
      "\n",
      "self = <__main__.TestConv2D object at 0x000002430FFF94D0>\n",
      "inputs = tensor([[[[-0.7242,  0.5520,  1.7713,  0.9649],\n",
      "          [-0.4341, -0.8966,  0.1245,  0.0645],\n",
      "          [ 0.0583, -0...,  1.3643,  0.4532],\n",
      "          [-1.3859, -0.4927,  0.9672, -0.6518],\n",
      "          [-2.3218, -1.5389, -1.1958,  0.3439]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_conv2d_bias_padding_stride_success\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, inputs, weights, bias):\u001b[90m\u001b[39;49;00m\n",
      ">       result = convolution2D(inputs, weights, bias, padding=\u001b[94m5\u001b[39;49;00m, stride=\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\106665805.py\u001b[0m:21: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "input = tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000,  0.000...0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603]), stride = 2, padding = 5, dilation = 1, groups = 1\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mconvolution2D\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        weights: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        bias: Optional[Tensor] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        stride: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        padding: Optional[Union[\u001b[96mint\u001b[39;49;00m, Union[Tuple, \u001b[96mstr\u001b[39;49;00m]]] = \u001b[94m0\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        dilation: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        groups: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        batch_size, in_channels, input_height, input_width = \u001b[96minput\u001b[39;49;00m.shape\u001b[90m\u001b[39;49;00m\n",
      "        out_channels, in_channels_groups, weights_height, weights_width = weights.shape\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m = nn.functional.pad(\u001b[96minput\u001b[39;49;00m, (padding, padding, padding, padding))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result_height = (\u001b[90m\u001b[39;49;00m\n",
      "            input_height + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_height - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result_width = (\u001b[90m\u001b[39;49;00m\n",
      "            input_width + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_width - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        grouped_channels = out_channels // groups \u001b[94mif\u001b[39;49;00m groups \u001b[94melse\u001b[39;49;00m out_channels\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = torch.zeros((batch_size, grouped_channels, result_height, result_width))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m batch \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(batch_size):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m channel \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(out_channels):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m2\u001b[39;49;00m] - dilation*(weights_height - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m3\u001b[39;49;00m] - dilation*(weights_width - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride\u001b[90m\u001b[39;49;00m\n",
      "                    ):\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mfor\u001b[39;49;00m group \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(grouped_channels):\u001b[90m\u001b[39;49;00m\n",
      "                            d = \u001b[96minput\u001b[39;49;00m[\u001b[90m\u001b[39;49;00m\n",
      "                                batch, :, i : i + weights_height, j : j + weights_width\u001b[90m\u001b[39;49;00m\n",
      "                            ]\u001b[90m\u001b[39;49;00m\n",
      "                            result[batch, group, i // stride, j // stride] = (\u001b[90m\u001b[39;49;00m\n",
      ">                               d * weights[channel]\u001b[90m\u001b[39;49;00m\n",
      "                            ).sum()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                           RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\303552282.py\u001b[0m:37: RuntimeError\n",
      "\u001b[31m\u001b[1m___________________ TestConv2D.test_conv2d_bias_padding_stride_dilation_success ___________________\u001b[0m\n",
      "\n",
      "self = <__main__.TestConv2D object at 0x000002430FF65550>\n",
      "inputs = tensor([[[[-0.7242,  0.5520,  1.7713,  0.9649],\n",
      "          [-0.4341, -0.8966,  0.1245,  0.0645],\n",
      "          [ 0.0583, -0...,  1.3643,  0.4532],\n",
      "          [-1.3859, -0.4927,  0.9672, -0.6518],\n",
      "          [-2.3218, -1.5389, -1.1958,  0.3439]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_conv2d_bias_padding_stride_dilation_success\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, inputs, weights, bias):\u001b[90m\u001b[39;49;00m\n",
      ">       result = convolution2D(inputs, weights, bias, padding=\u001b[94m5\u001b[39;49;00m, stride=\u001b[94m2\u001b[39;49;00m, dilation=\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\106665805.py\u001b[0m:26: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "input = tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000,  0.000...0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n",
      "weights = tensor([[[[ 0.0609, -0.2714, -0.6916],\n",
      "          [ 0.3006,  0.7230, -0.6429],\n",
      "          [-0.3125, -0.4023, -1.2953]],\n",
      "\n",
      "         [[ 0.5244, -0.8564, -0.3703],\n",
      "          [-2.5459, -1.3926, -0.4236],\n",
      "          [-1.9287, -0.8121, -0.9855]]]])\n",
      "bias = tensor([-2.1603]), stride = 2, padding = 5, dilation = 2, groups = 1\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mconvolution2D\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        weights: Tensor,\u001b[90m\u001b[39;49;00m\n",
      "        bias: Optional[Tensor] = \u001b[94mNone\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        stride: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        padding: Optional[Union[\u001b[96mint\u001b[39;49;00m, Union[Tuple, \u001b[96mstr\u001b[39;49;00m]]] = \u001b[94m0\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        dilation: Optional[Union[\u001b[96mint\u001b[39;49;00m, Tuple]] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        groups: Optional[\u001b[96mint\u001b[39;49;00m] = \u001b[94m1\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    ) -> Tensor:\u001b[90m\u001b[39;49;00m\n",
      "        batch_size, in_channels, input_height, input_width = \u001b[96minput\u001b[39;49;00m.shape\u001b[90m\u001b[39;49;00m\n",
      "        out_channels, in_channels_groups, weights_height, weights_width = weights.shape\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[96minput\u001b[39;49;00m = nn.functional.pad(\u001b[96minput\u001b[39;49;00m, (padding, padding, padding, padding))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result_height = (\u001b[90m\u001b[39;49;00m\n",
      "            input_height + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_height - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        result_width = (\u001b[90m\u001b[39;49;00m\n",
      "            input_width + \u001b[94m2\u001b[39;49;00m * padding - dilation * (weights_width - \u001b[94m1\u001b[39;49;00m) - \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        ) // stride + \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        grouped_channels = out_channels // groups \u001b[94mif\u001b[39;49;00m groups \u001b[94melse\u001b[39;49;00m out_channels\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        result = torch.zeros((batch_size, grouped_channels, result_height, result_width))\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m batch \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(batch_size):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mfor\u001b[39;49;00m channel \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(out_channels):\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m2\u001b[39;49;00m] - dilation*(weights_height - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride):\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94m0\u001b[39;49;00m, \u001b[96minput\u001b[39;49;00m.shape[\u001b[94m3\u001b[39;49;00m] - dilation*(weights_width - \u001b[94m1\u001b[39;49;00m) + \u001b[94m1\u001b[39;49;00m, stride\u001b[90m\u001b[39;49;00m\n",
      "                    ):\u001b[90m\u001b[39;49;00m\n",
      "                        \u001b[94mfor\u001b[39;49;00m group \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(grouped_channels):\u001b[90m\u001b[39;49;00m\n",
      "                            d = \u001b[96minput\u001b[39;49;00m[\u001b[90m\u001b[39;49;00m\n",
      "                                batch, :, i : i + weights_height, j : j + weights_width\u001b[90m\u001b[39;49;00m\n",
      "                            ]\u001b[90m\u001b[39;49;00m\n",
      ">                           result[batch, group, i // stride, j // stride] = (\u001b[90m\u001b[39;49;00m\n",
      "                                d * weights[channel]\u001b[90m\u001b[39;49;00m\n",
      "                            ).sum()\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE                           IndexError: index 5 is out of bounds for dimension 3 with size 5\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18384\\303552282.py\u001b[0m:36: IndexError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_fe9db8d6d7794447b06c979307742dcf.py::\u001b[1mTestConv2D::test_conv2d_success\u001b[0m - RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dim...\n",
      "\u001b[31mFAILED\u001b[0m t_fe9db8d6d7794447b06c979307742dcf.py::\u001b[1mTestConv2D::test_conv2d_bias_success\u001b[0m - RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dim...\n",
      "\u001b[31mFAILED\u001b[0m t_fe9db8d6d7794447b06c979307742dcf.py::\u001b[1mTestConv2D::test_conv2d_bias_padding_success\u001b[0m - RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dim...\n",
      "\u001b[31mFAILED\u001b[0m t_fe9db8d6d7794447b06c979307742dcf.py::\u001b[1mTestConv2D::test_conv2d_bias_padding_stride_success\u001b[0m - RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dim...\n",
      "\u001b[31mFAILED\u001b[0m t_fe9db8d6d7794447b06c979307742dcf.py::\u001b[1mTestConv2D::test_conv2d_bias_padding_stride_dilation_success\u001b[0m - IndexError: index 5 is out of bounds for dimension 3 with size 5\n",
      "\u001b[31m\u001b[31m\u001b[1m5 failed\u001b[0m\u001b[31m in 0.19s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.usefixtures('inputs')\n",
    "@pytest.mark.usefixtures('weights')\n",
    "@pytest.mark.usefixtures('bias')\n",
    "class TestConv2D:\n",
    "    def test_conv2d_success(self, inputs, weights):\n",
    "        result = convolution2D(inputs, weights)\n",
    "        expected_result = nn.functional.conv2d(inputs, weights)\n",
    "        assert torch.allclose(expected_result, result)\n",
    "        \n",
    "    def test_conv2d_bias_success(self, inputs, weights, bias):\n",
    "        result = convolution2D(inputs, weights, bias)\n",
    "        expected_result = nn.functional.conv2d(inputs, weights, bias)\n",
    "        assert torch.allclose(expected_result, result)\n",
    "        \n",
    "    def test_conv2d_bias_padding_success(self, inputs, weights, bias):\n",
    "        result = convolution2D(inputs, weights, bias, padding=5)\n",
    "        expected_result = nn.functional.conv2d(inputs, weights, bias, padding=5)\n",
    "        assert torch.allclose(expected_result, result)\n",
    "        \n",
    "    def test_conv2d_bias_padding_stride_success(self, inputs, weights, bias):\n",
    "        result = convolution2D(inputs, weights, bias, padding=5, stride=2)\n",
    "        expected_result = nn.functional.conv2d(inputs, weights, bias, padding=5, stride=2)\n",
    "        assert torch.allclose(expected_result, result)\n",
    "        \n",
    "    def test_conv2d_bias_padding_stride_dilation_success(self, inputs, weights, bias):\n",
    "        result = convolution2D(inputs, weights, bias, padding=5, stride=2, dilation=2)\n",
    "        expected_result = nn.functional.conv2d(inputs, weights, bias, padding=5, stride=2, dilation=2)\n",
    "        assert torch.allclose(expected_result, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import ipytest\n",
    "import numpy as np\n",
    "import pytest\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.common_types import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution3D(\n",
    "    input: Tensor,\n",
    "    weights: Tensor,\n",
    "    bias: Optional[Tensor] = None,\n",
    "    stride: Optional[Union[int, Tuple]] = 1,\n",
    "    padding: Optional[Union[int, Union[Tuple, str]]] = 0,\n",
    "    dilation: Optional[Union[int, Tuple]] = 1,\n",
    "    groups: Optional[int] = 1,\n",
    ") -> Tensor:\n",
    "    batch_size, in_channels, input_height, input_width, input_depth = input.shape\n",
    "    (\n",
    "        out_channels,\n",
    "        in_channels_groups,\n",
    "        weights_height,\n",
    "        weights_width,\n",
    "        weights_depth,\n",
    "    ) = weights.shape\n",
    "\n",
    "    input = nn.functional.pad(\n",
    "        input, (padding, padding, padding, padding, padding, padding), mode=\"constant\"\n",
    "    )\n",
    "\n",
    "    result_height = (\n",
    "        input_height + 2 * padding - dilation * (weights_height - 1) - 1\n",
    "    ) // stride + 1\n",
    "    result_width = (\n",
    "        input_width + 2 * padding - dilation * (weights_width - 1) - 1\n",
    "    ) // stride + 1\n",
    "    result_depth = (\n",
    "        input_depth + 2 * padding - dilation * (weights_depth - 1) - 1\n",
    "    ) // stride + 1\n",
    "\n",
    "    grouped_channels = out_channels // groups if groups else out_channels\n",
    "\n",
    "    result = torch.zeros(\n",
    "        (batch_size, grouped_channels, result_height, result_width, result_depth)\n",
    "    )\n",
    "\n",
    "    for batch in range(0, batch_size):\n",
    "        for channel in range(out_channels):\n",
    "            for i in range(0, input.shape[2] - (weights_height - 1), stride):\n",
    "                for j in range(0, input.shape[3] - (weights_width - 1), stride):\n",
    "                    for k in range(0, input.shape[4] - (weights_depth - 1), stride):\n",
    "                        for group in range(grouped_channels):\n",
    "                            result[:, group, i // stride, j // stride, k // stride] = (\n",
    "                                weights[batch]\n",
    "                                * input[\n",
    "                                    batch,\n",
    "                                    :,\n",
    "                                    i : i + weights_height,\n",
    "                                    j : j + weights_width,\n",
    "                                    k : k + weights_depth,\n",
    "                                ]\n",
    "                            ).sum()\n",
    "            result[batch] += bias[channel] if bias else 0\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture(scope='class')\n",
    "def inputs():\n",
    "    return torch.randn(1, 2, 4, 4, 4)\n",
    "\n",
    "@pytest.fixture(scope='class')\n",
    "def weights():\n",
    "    return torch.randn(1, 2, 3, 3, 3)\n",
    "\n",
    "@pytest.fixture(scope='class')\n",
    "def bias():\n",
    "    return torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "\n",
    "@pytest.mark.usefixtures('inputs')\n",
    "@pytest.mark.usefixtures('weights')\n",
    "@pytest.mark.usefixtures('bias')\n",
    "class TestConv2D:\n",
    "    def test_conv2d_success(self, inputs, weights):\n",
    "        result = convolution3D(inputs, weights)\n",
    "        expected_result = nn.functional.conv3d(inputs, weights)\n",
    "        assert torch.allclose(expected_result, result)\n",
    "        \n",
    "    def test_conv2d_bias_success(self, inputs, weights, bias):\n",
    "        result = convolution3D(inputs, weights, bias)\n",
    "        expected_result = nn.functional.conv3d(inputs, weights, bias)\n",
    "        assert torch.allclose(expected_result, result)\n",
    "        \n",
    "    def test_conv2d_bias_padding_success(self, inputs, weights, bias):\n",
    "        result = convolution3D(inputs, weights, bias, padding=2)\n",
    "        expected_result = nn.functional.conv3d(inputs, weights, bias, padding=2)\n",
    "        assert torch.allclose(expected_result, result)\n",
    "        \n",
    "    def test_conv2d_bias_padding_stride_success(self, inputs, weights, bias):\n",
    "        result = convolution3D(inputs, weights, bias, padding=1, stride=2)\n",
    "        expected_result = nn.functional.conv3d(inputs, weights, bias, padding=1, stride=2)\n",
    "        assert torch.allclose(expected_result, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
